llm:
  mode: ollama

ollama:
  model: mistral # Required Model to use.
                               # Note: Ollama Models are listed here: https://ollama.ai/library
                               #       Be sure to pull the model to your Ollama server
  api_base: http://localhost:11434
